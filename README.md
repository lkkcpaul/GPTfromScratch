# GPTfromScratch
This is my exercise for building GPT from scratch to better understand self attention and transformer.
I mainly followed this tutorial https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy.

I first trained on the Chinese text 論語 ("Analects of Confucius") and then on 紅樓夢 ("Dream of Red Mansion"), 
the reason being Chinese tokenization is easier and there's less Chinese examples out there. 

This runs on the GPU (NVidia GeForce RTX 2060 ) on my laptop.
The output follows the style of the input text and makes some sensible sentences here and there. 
This is not a fine-tuning of a pretrained model. 

Feel free to use my code and I'm open to suggestions. 
